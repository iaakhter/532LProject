{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pylab\n",
    "import utils\n",
    "import os.path\n",
    "from PIL import Image\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models, transforms\n",
    "import torch\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('track', 'tags')\n",
      "('album', 'tags')\n",
      "('artist', 'tags')\n",
      "('track', 'genres')\n",
      "('track', 'genres_all')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iaakhter/Documents/UBC/Winter22017/532L/DreamingInMusic/utils.py:218: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  'category', categories=SUBSETS, ordered=True)\n"
     ]
    }
   ],
   "source": [
    "# Load metadata and features.\n",
    "tracks = utils.load('data/fma_metadata/tracks.csv')\n",
    "genres = utils.load('data/fma_metadata/genres.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hip-Hop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       " -32.9089 -32.9568 -29.7125  ...  -35.9170 -33.7426 -38.4198\n",
       " -31.5367 -30.3374 -27.5425  ...  -32.6929 -33.3952 -34.4451\n",
       " -36.5218 -30.7120 -27.8699  ...  -34.9015 -33.7515 -35.1991\n",
       "            ...               â‹±              ...            \n",
       " -38.5109 -36.4188 -30.5375  ...  -41.9084 -42.2655 -40.9039\n",
       " -35.6341 -34.0811 -29.7336  ...  -39.4294 -36.0721 -36.9820\n",
       " -32.6071 -36.9618 -30.5371  ...  -36.3189 -34.3709 -34.7984\n",
       "[torch.DoubleTensor of size 1x224x224]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# return the first genreId of a track (a track can have multiple genres)\n",
    "# rock genre id is 12, hiphop is 21 and pop is 10\n",
    "def getGenreId(trackId):\n",
    "    if len(tracks['track','genres'][trackId]) >= 1:\n",
    "        return tracks['track','genres'][trackId][0]\n",
    "    else:\n",
    "        # if the track does not have a genre\n",
    "        return None\n",
    "    \n",
    "# Define a global transformer to appropriately scale images and subsequently convert them to a Tensor.\n",
    "array_size = 224\n",
    "audioDirectory = \"data/fma_small/\"\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/7525214/how-to-scale-a-numpy-array\n",
    "def cropArray(x, newSize):\n",
    "    midRow = x.shape[0]//2\n",
    "    midCol = x.shape[1]//2\n",
    "    x = x[midRow - newSize//2:midRow + newSize//2, midCol - newSize//2:midCol + newSize//2]\n",
    "    return x\n",
    "\n",
    "\n",
    "def trackExists(trackIdNum):\n",
    "    trackId = str(trackIdNum)\n",
    "    while(len(trackId) < 6):\n",
    "        trackId = \"0\" + trackId\n",
    "    filename = trackId[0:3]+\"/\"+trackId\n",
    "    audioFilename = audioDirectory + filename + \".mp3\"\n",
    "    if os.path.isfile(audioFilename):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# return processed spectrogram of a track\n",
    "def loadSpectro(trackIdNum):\n",
    "    \"\"\"\n",
    "    Simple function to load and preprocess the image.\n",
    "\n",
    "    1. Open the image.\n",
    "    2. Scale/crop it and convert it to a float tensor.\n",
    "    3. Convert it to a variable (all inputs to PyTorch models must be variables).\n",
    "    4. Add another dimension to the start of the Tensor (b/c VGG expects a batch).\n",
    "    5. Move the variable onto the GPU.\n",
    "    \"\"\"\n",
    "    trackId = str(trackIdNum)\n",
    "    while(len(trackId) < 6):\n",
    "        trackId = \"0\" + trackId\n",
    "    filename = trackId[0:3]+\"/\"+trackId\n",
    "    audioFilename = audioDirectory + filename + \".mp3\"\n",
    "    if trackExists(trackIdNum):\n",
    "        x, sr = librosa.load(audioFilename, sr=None, mono=True)\n",
    "        #Convert audio to a complex valued spectrogram\n",
    "        spectro = librosa.core.stft(x)\n",
    "\n",
    "        #Separate out amplitude and phase from complex valued spectrogram\n",
    "        mag, phase = librosa.core.magphase(spectro)\n",
    "        #print (\"mag\", mag)\n",
    "        #print (\"phase\",phase)\n",
    "\n",
    "        #Get the decibal version from power spectrogram\n",
    "        #This is the value that should be stored for training\n",
    "        powerToDb = librosa.power_to_db(mag, ref=np.max)\n",
    "        scaledSpectro = cropArray(powerToDb, array_size)\n",
    "        spectroTensor = torch.from_numpy(scaledSpectro).type(torch.DoubleTensor)\n",
    "        spectroVar = Variable(spectroTensor).unsqueeze(0)\n",
    "        return spectroVar\n",
    "    else:\n",
    "        print(\"Audio does not exist\")\n",
    "        return None\n",
    "genres\n",
    "trackId = 2\n",
    "print(genres['title'][getGenreId(trackId)])\n",
    "loadSpectro(trackId)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/srviest/char-cnn-pytorch/blob/master/model_CharCNN2D.py\n",
    "class SpectroCNN(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(SpectroCNN, self).__init__()    \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1)),\n",
    "            nn.ReLU()\n",
    "        ).double()\n",
    "\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(4, 4), stride=(1, 1))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(222, 222, kernel_size=(2, 2), stride=(1,1)),\n",
    "            nn.ReLU()\n",
    "        ).double()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(200, 200), stride=(4, 4))\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(1800, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5)\n",
    "        ).double()\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5)\n",
    "        ).double()\n",
    "        self.fc3 =nn.Linear(1024, output_size).double()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "            # nn.LogSoftmax()\n",
    "\n",
    "        # self.inference_log_softmax = InferenceBatchLogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        debug=False\n",
    "        x = x.unsqueeze(1)\n",
    "        if debug:\n",
    "            print('x.size()', x.size())\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        if debug:\n",
    "            print('x after conv1', x.size())\n",
    "\n",
    "        x = x.transpose(1,3)\n",
    "        if debug:\n",
    "            print('x after transpose', x.size())\n",
    "\n",
    "        x = self.maxpool1(x)\n",
    "        if debug:\n",
    "            print('x after maxpool1', x.size())\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        if debug:\n",
    "            print('x after conv2', x.size())\n",
    "\n",
    "        x = x.transpose(1,3)\n",
    "        if debug:\n",
    "            print('x after transpose', x.size())\n",
    "\n",
    "        x = self.maxpool2(x)\n",
    "        if debug:\n",
    "            print('x after maxpool2', x.size())\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        if debug:\n",
    "            print('Collapse x:, ', x.size())\n",
    "\n",
    "        x = self.fc1(x).type(torch.DoubleTensor)\n",
    "        if debug:\n",
    "            print('FC1: ', x.size())\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        if debug:\n",
    "            print('FC2: ', x.size())\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        if debug:\n",
    "            print('x: ', x.size())\n",
    "\n",
    "        x = self.softmax(x)\n",
    "        # x = self.inference_log_softmax(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/myEnv2/lib/python3.6/site-packages/ipykernel_launcher.py:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.423885960115509\n",
      "Starting iteration:  1\n",
      "1 1.8480728067984742\n",
      "Starting iteration:  2\n",
      "2 0.23472917934248896\n",
      "Starting iteration:  3\n",
      "3 0.23104908751567466\n",
      "Starting iteration:  4\n",
      "4 0.2310490601853169\n",
      "Starting iteration:  5\n",
      "5 0.23104906018531735\n",
      "Starting iteration:  6\n",
      "6 0.23104906018531504\n",
      "Starting iteration:  7\n",
      "7 0.23104908994478238\n",
      "Starting iteration:  8\n",
      "8 0.23104906018531504\n",
      "Starting iteration:  9\n",
      "9 0.23104906018531504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.423885960115509,\n",
       " 1.8480728067984742,\n",
       " 0.23472917934248896,\n",
       " 0.23104908751567466,\n",
       " 0.2310490601853169,\n",
       " 0.23104906018531735,\n",
       " 0.23104906018531504,\n",
       " 0.23104908994478238,\n",
       " 0.23104906018531504,\n",
       " 0.23104906018531504]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids = [2,5,10,140,141,148,182,190,193,194]\n",
    "tempCat = {'Pop':0, 'Rock':1, 'Hip-Hop':2}\n",
    "\n",
    "def create_training(start, end):\n",
    "    training_input = [loadSpectro(train_id).squeeze() for train_id in train_ids[start:end]\n",
    "                      if trackExists(train_id)]\n",
    "\n",
    "    # The output data is prepared by representing each output as a binary vector of categories\n",
    "    training_output = []\n",
    "    for i in range(start,min(len(train_ids),end)):\n",
    "        if not(trackExists(train_ids[i])):\n",
    "            continue\n",
    "        genre = genres['title'][getGenreId(trackId)]\n",
    "        training_vector = np.zeros(len(tempCat))\n",
    "        training_vector[tempCat[genre]] = 1\n",
    "        training_output.append(training_vector)\n",
    "    training_output = Variable(torch.FloatTensor(training_output))\n",
    "    \n",
    "    return training_input, training_output\n",
    "\n",
    "def train(model, learning_rate=0.0001, batch_size=1, epochs=1):\n",
    "    \"\"\"\n",
    "    Training function which takes as input a model, a learning rate and a batch size.\n",
    "  \n",
    "    After completing a full pass over the data, the function exists, and the input model will be trained.\n",
    "    \"\"\"\n",
    "    # Define the criterion and optimizer.\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Keep track of the losses, for the purposes of plotting.\n",
    "    train_losses = []\n",
    "\n",
    "    # Determine number of minibatches\n",
    "    num_iter = epochs * len(train_ids)//batch_size \n",
    "    for i in range(num_iter):\n",
    "        print(\"Starting iteration: \", i)\n",
    "        \n",
    "        start_idx = i * batch_size % len(train_ids)\n",
    "        \n",
    "        training_input, training_output = create_training(start_idx, start_idx + batch_size)\n",
    "\n",
    "        # Retrieve the next batch of training data.\n",
    "        x = torch.stack(training_input)\n",
    "        y = training_output.type(torch.DoubleTensor)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        # Zero gradients, perform backwards pass and update model weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        \n",
    "        del x,y,y_pred\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            train_losses.append(loss.data[0])\n",
    "            print(i, train_losses[-1])\n",
    "        \n",
    "    return train_losses \n",
    "\n",
    "# Finally train the model\n",
    "numClasses = 3\n",
    "model = SpectroCNN(numClasses)\n",
    "model.train()\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
